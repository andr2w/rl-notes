{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72e9b44",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "- Policy 策略: 智能体会用策略来选取下一步的动作\n",
    "- Value function 价值函数: 用价值函数来对当前状态进行评估. 价值函数用于评估智能体进入某个状态后, 可以对后面的奖励带来多大的影响. 价值函数数值越大, 说明智能体进入这个状态越有利.\n",
    "- Model 模型: 模型表示智能体对环境的状态进行理解, 它决定了环境中世界的运行方式."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72084971",
   "metadata": {},
   "source": [
    "#### Policy\n",
    "\n",
    "策略是智能体的动作模型, 它决定了智能体的动作. 策略是一个函数, 用于把输入的状态变成动作. \n",
    "\n",
    "- 随机性策略: stochastic policy $\\pi(a\\mid s) = p(a_t = a\\mid s_t = s)$. 输入一个状态$s$, 输出一个概率. \n",
    "- 确定性策略: deterministic policy 智能体直接采用最有可能的动作 $a^* = \\text{argmax}_{a}\\pi (a\\mid s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f6bda",
   "metadata": {},
   "source": [
    "#### Value function\n",
    "\n",
    "价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8979b",
   "metadata": {},
   "source": [
    "#### 基于价值的智能体与基于策略的智能体\n",
    "\n",
    "\n",
    "根据智能体学习的事物不同，我们可以把智能体进行归类。基于价值的智能体（value-based agent）显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。基于策略的智能体（policy-based agent）直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了演员-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee00b2",
   "metadata": {},
   "source": [
    "- 从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 \n",
    "    - 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。\n",
    "        - 基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。\n",
    "    - 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。\n",
    "        - 基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。\n",
    "            -  基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等.          \n",
    "- 演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd3f920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6fc96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e908a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39def6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cde330a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be04e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a5e1328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0f8a7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad539a5e",
   "metadata": {},
   "source": [
    "### Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d75d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34261eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totoal reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "done = False \n",
    "score = 0\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    score += reward\n",
    "    \n",
    "print('totoal reward: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39be2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 totoal reward: 35.0\n",
      "epoch: 2 totoal reward: 17.0\n",
      "epoch: 3 totoal reward: 28.0\n",
      "epoch: 4 totoal reward: 19.0\n",
      "epoch: 5 totoal reward: 25.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5+1):\n",
    "    done = False \n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('epoch: {} totoal reward: {}'.format(epoch, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb1924",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "Env is a class, it contains \n",
    "\n",
    "- action space\n",
    "- states\n",
    "- reward\n",
    "- action space.sample\n",
    "- action.sample\n",
    "- steps (how many steps in this turn)\n",
    "- done or not done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
