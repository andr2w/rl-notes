{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f792e33",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "强化学习的目的是什么? \n",
    "\n",
    "- 想要训练一个agent, 这个agent是来去做动作. \n",
    "- agent通过做动作来最大化reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a8c180",
   "metadata": {},
   "source": [
    "- Policy $\\pi$ is a network with parameter $\\theta$\n",
    "    - Input: the observation of machine represented as a vector of a matrix\n",
    "    - Output: each action corresponds to a neuron in output layer\n",
    "- Trajectory $\\tau = \\{s_1, a_1, s_2, a_2, \\cdots, s_T, a_T\\}$\n",
    "    - $p_\\theta(\\tau) = p(s_1)p_\\theta(a_1 \\mid s_1)p(s_2 \\mid s_1, a_1)p_\\theta(a_2\\mid s_2)p_(s_3\\mid s_2, a_2) \\cdots$\n",
    "    - $p_\\theta(\\tau) = p(s_1) \\prod_{t=1}^T p_\\theta(a_t \\mid s_t) p(s_{t+1}\\mid s_t, a_t)$\n",
    "        - $p(s_{t+1}\\mid s_t, a_t)$ the state transction function.\n",
    "        - $p_\\theta(a_t \\mid s_t)$ the policy function\n",
    "- Reward $R(\\tau) = \\sum_{t=1}^T r_t$, $R$ is a random variable. \n",
    "    - Only can compute $\\bar{R}_\\theta = \\sum_{\\tau}R(\\tau)p_\\theta(\\tau) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} [R(\\tau)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd22d66",
   "metadata": {},
   "source": [
    "#### Maximize the reward\n",
    "##### Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9e5e9",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)] = \\sum_\\tau R(\\tau)p_\\theta(\\tau)$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla\\bar{R}_\\theta &= \\sum_\\tau R(\\tau)\\nabla p_\\theta(\\tau) \\\\\n",
    "                     &= \\sum_\\tau R(\\tau) p_\\theta(\\tau) \\nabla \\log p_\\theta(\\tau) \\\\\n",
    "                     &= \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla \\log p_\\theta(\\tau)] \\\\\n",
    "                     &\\text{Note:} \\sum_\\tau \\text{ and } p_\\theta(\\tau)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Note that:\n",
    "$$\\nabla f(x) = f(x)\\nabla \\log f(x)$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\theta(\\tau) &= p(s_1)p_\\theta(a_1 \\mid s_1)p(s_2 \\mid s_1, a_1)p_\\theta(a_2\\mid s_2)p_(s_3\\mid s_2, a_2) \\cdots \\\\\n",
    "               &= p(s_1) \\prod_{t=1}^T p_\\theta(a_t \\mid s_t) p(s_{t+1}\\mid s_t, a_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "**policy function:**\n",
    "$$p_\\theta(a_t \\mid s_t)$$\n",
    "---\n",
    "总结: \n",
    "\n",
    "- 在一组参数$\\theta$下, 会有很多个trajectory $\\tau$. 出现的每一个trajectory $\\tau$的概率是$p_\\theta(\\tau)$, 通过链式法则, 对于$\\theta$求梯度.\n",
    "- 实际上, 期望值$\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)[R(\\tau) \\nabla \\log p_\\theta(\\tau)]}$ 无法计算, 所以我们用采样$N$个$\\tau$并计算每一个值, 把每一个值加起来, 就可以得到梯度.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla \\log p_\\theta(\\tau)] &= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla \\log p_\\theta(\\tau^n) \\\\\n",
    "                                                                         &= \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n) \\nabla \\log p_\\theta(a_t^n \\mid s_t^n)\n",
    "\\end{align*}\n",
    "$$\n",
    "采样$N$个$\\tau$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e73bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\\nabla \\log p_\\theta(\\tau)$的具体计算过程可以写为:\n",
    "\n",
    "- $\\sum_{t=1}^T\\log p_\\theta(a_t\\mid s_t)$ is the policy function.\n",
    "- $\\sum_{t=1}^T \\log p(s_{t+1}\\mid s_t, a_t)$ is the state transcation function.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\log p_\\theta(\\tau) &= \\nabla \\left(\\log p(s_1) + \\sum_{t=1}^{T} \\log p_\\theta(a_t \\mid s_t) + \\sum_{t=1}^T \\log p(s_{t+1}\\mid s_t, a_t) \\right)\\\\\n",
    "& = \\nabla \\log p(s_1) + \\nabla \\sum_{t=1}^T \\log p_\\theta(a_t \\mid s_t) + \\nabla \\sum_{t=1}^T \\log p(s_{t+1}\\mid s_t, a_t) \\\\\n",
    "& = \\nabla \\sum_{t=1}^T \\log p_\\theta(a_t \\mid s_t) \\\\\n",
    "& = \\sum_{t=1}^T \\nabla \\log p_\\theta(a_t \\mid s_t) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df6ca7",
   "metadata": {},
   "source": [
    "带入进$\\nabla \\bar{R}_\\theta = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla p_\\theta(\\tau)]$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\bar{R}_\\theta &= \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla p_\\theta(\\tau)] \\\\\n",
    "                      &= \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n)\\nabla \\log p_\\theta(a_t^n \\mid s_t^n) \\\\\n",
    "                      &= \\frac{1}{N}\\sum_{n=1}^N\\sum_{t=1}^{T_n} R(\\tau^n) \\nabla \\log p_\\theta(a_t^n \\mid s_t^n)\n",
    "\\end{align*}$$\n",
    "\n",
    "即可得到上述式子.\n",
    "\n",
    "最后更新方式如下:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\eta \\nabla \\bar{R}_\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020cff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
