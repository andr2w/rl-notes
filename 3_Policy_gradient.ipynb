{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a8c180",
   "metadata": {},
   "source": [
    "- Policy $\\pi$ is a network with parameter $\\theta$\n",
    "    - Input: the observation of machine represented as a vector of a matrix\n",
    "    - Output: each action corresponds to a neuron in output layer\n",
    "- Trajectory $\\tau = \\{s_1, a_1, s_2, a_2, \\cdots, s_T, a_T\\}$\n",
    "    - $p_\\theta(\\tau) = p(s_1)p_\\theta(a_1 \\mid s_1)p(s_2 \\mid s_1, a_1)p_\\theta(a_2\\mid s_2)p_(s_3\\mid s_2, a_2) \\cdots$\n",
    "    - $p_\\theta(\\tau) = p(s_1) \\prod_{t=1}^T p_\\theta(a_t \\mid s_t) p(s_{t+1}\\mid s_t, a_t)$\n",
    "        - $p(s_{t+1}\\mid s_t, a_t)$ the state transction function.\n",
    "        - $p_\\theta(a_t \\mid s_t)$ the policy function\n",
    "- Reward $R(\\tau) = \\sum_{t=1}^T r_t$, $R$ is a random variable. \n",
    "    - Only can compute $\\bar{R}_\\theta = \\sum_{\\tau}R(\\tau)p_\\theta(\\tau) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} [R(\\tau)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd22d66",
   "metadata": {},
   "source": [
    "#### Maximize the reward\n",
    "##### Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9e5e9",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)] = \\sum_\\tau R(\\tau)p_\\theta(\\tau)$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla\\bar{R}_\\theta &= \\sum_\\tau R(\\tau)\\nabla p_\\theta(\\tau) \\\\\n",
    "                       &= \\sum_\\tau R(\\tau)p_\\theta(\\tau)\\frac{\\nabla p_\\theta(\\tau)}{p_\\theta(\\tau)} \\\\\n",
    "                       &= \\sum_\\tau R(\\tau)p_\\theta(\\tau)\\frac{\\nabla p_\\theta(\\tau) p_\\theta(\\tau)}{p_\\theta(\\tau) p_\\theta(\\tau)} \\\\\n",
    "                       &= \\sum_\\tau R(\\tau)p_\\theta(\\tau)\\nabla \\log p_\\theta(\\tau) \\\\\n",
    "                       &= \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla \\log p_\\theta(\\tau)] \\\\\n",
    "                       &\\text{Note:} \\sum_\\tau \\text{ and } p_\\theta(\\tau)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "$$\\nabla f(x) = f(x)\\nabla \\log f(x)$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\theta(\\tau) &= p(s_1)p_\\theta(a_1 \\mid s_1)p(s_2 \\mid s_1, a_1)p_\\theta(a_2\\mid s_2)p_(s_3\\mid s_2, a_2) \\cdots \\\\\n",
    "               &= p(s_1) \\prod_{t=1}^T p_\\theta(a_t \\mid s_t) p(s_{t+1}\\mid s_t, a_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "**policy function:**\n",
    "$$p_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "总结: 在一组参数$\\theta$下, 会有很多个trajectory $\\tau$. 出现的每一个trajectory $\\tau$的概率是$p_\\theta(\\tau)$, 通过链式法则, 对于$\\theta$求梯度.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)\\nabla \\log p_\\theta(\\tau)] &= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla \\log p_\\theta(\\tau^n) \\\\\n",
    "                                                                         &= \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} R(\\tau^n) \\nabla \\log p_\\theta(a_t^n \\mid s_t^n)\n",
    "\\end{align*}\n",
    "$$\n",
    "采样$N$个$\\tau$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
